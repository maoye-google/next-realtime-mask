Real-time Camera Masking Application: Feature & Architecture Plan
1. Introduction
This document outlines the design and architecture for a real-time camera masking web application. The application will capture video from a user's USB camera, identify a target object based on a text prompt, and apply a visual mask (grey out or border highlight) to that object in real-time. The system is designed to be modular, scalable, and developer-friendly, utilizing a microservices-style architecture orchestrated with Docker.
2. Core Feature Requirements
2.1. User Interface (UI)
* Real-time Camera View: Display the raw, unprocessed video stream directly from the user's camera.
* Processed Video View: Display the video stream after processing, with the designated mask applied to the tracked object.
* Control Video View: Display the original, unprocessed video frame that directly corresponds in time to the frame shown in the "Processed Video" view. This allows for a direct comparison and visualization of the processing delay.
* Masking Target Input: A text field where the user can enter a prompt describing the object to be detected (e.g., "a person's face", "a blue coffee mug", "my left hand").
* Masking Format Selection: A choice between two masking styles:
   * Grey out: The detected object area is covered with a semi-transparent grey mask.
   * Border Highlight: A colored border is drawn around the detected object's segmentation mask.
* Controls:
   * Start/Stop Button: To initiate and terminate the detection and tracking process. Everytime when we start the detection, all old result should be deleted.
   * Snapshot Frequency: A user-configurable setting (e.g., via a slider or input field) to define how often (in seconds) a snapshot is taken for initial object detection.
* Delay Metrics:
   * Current Delay: A real-time display of the latency (in seconds) between the "Real-time Camera" and the "Processed Video".
   * Average Delay: A running average of the processing latency.
2.2. Functional Requirements
* Camera Access: The application must request and gain access to the user's connected USB camera via standard browser APIs.
* Initial Object Detection:
   * When the user clicks "Start", the frontend periodically captures snapshots from the video stream.
   * These snapshots, along with the user's prompt, are sent to the backend for analysis.
   * A processing module uses a segmentation model (e.g., Segment Anything Model) to identify potential objects and Google Vertex AI to semantically match these objects against the user's prompt.
* Real-time Object Tracking:
   * Once an object is successfully identified, the system must switch from detection mode to a lower-latency tracking mode.
   * The backend will track the object's position across subsequent frames in real-time.
   * The tracking must persist as long as the object is visible in the frame. If the object is lost, the system can revert to the initial detection mode.
* Data Persistence: All processing results (e.g., timestamp, prompt, detection success, mask data) must be stored in a MongoDB database.
2.3. Development & Deployment
* Local Development Environment: The entire application stack (frontend, backend, processing module, database, etc.) must be launchable via a single docker compose up command.
* Modularity: The system must be composed of independent frontend, backend, and processing modules.
3. System Architecture
The application will be built using a distributed, microservices-oriented architecture to separate concerns and allow for independent scaling and development.
3.1. Architecture Diagram
Code snippet

graph TD
    subgraph User Browser
        A[Frontend - React.js]
    end


    subgraph Cloud/Local Docker Environment
        B[Backend - Python API]
        C[Processing Module - Python]
        D[Kafka]
        E[MongoDB]
        F[Vertex AI API]
    end


    A -- REST API (Initial Snapshot) --> B
    A -- WebSocket (Real-time Video Stream) --> B
    B -- Produces Msg (image, prompt) --> D[Topic: snapshot-requests]
    C -- Consumes Msg --> D
    C -- Calls for Analysis --> F
    C -- Produces Msg (result, mask) --> G[Topic: processing-results]
    B -- Consumes Msg --> G
    B -- Stores Result --> E
    B -- WebSocket (Processed Stream & Stats) --> A


    style F fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff


3.2. Component Breakdown
Component
	Technology
	Responsibilities
	realtime-mask-frontend
	React.js
	- Renders the UI. &lt;br> - Accesses the camera via navigator.mediaDevices.getUserMedia. &lt;br> - Manages WebSocket connections for streaming video to/from the backend. &lt;br> - Captures and sends initial snapshots via REST API. &lt;br> - Displays raw, processed, and control video feeds. &lt;br> - Calculates and displays average delay.
	realtime-mask-backend
	Python (FastAPI/Flask)
	- Provides REST API endpoints for the frontend. &lt;br> - Manages WebSocket connections for real-time video I/O. &lt;br> - Acts as a Kafka producer (for detection requests) and consumer (for results). &lt;br> - Once the object is found, performs real-time object tracking on the video stream (e.g., using OpenCV trackers). &lt;br> - Applies the visual mask to video frames. &lt;br> - Interfaces with MongoDB to store and delete results.
	realtime-mask-processing
	Python
	- A standalone Kafka consumer. &lt;br> - Subscribes to the snapshot-requests topic. &lt;br> - Uses a segmentation model (e.g., Segment Anything) to generate masks. &lt;br> - Calls Google Vertex AI Multimodal API to validate if a segment matches the text prompt. &lt;br> - Publishes the structured result (success/failure, mask coordinates) to the processing-results topic.
	- Calls the `segment-anything` service (via HTTP) to get object masks from an image. <br> - Calls Google Vertex AI Multimodal API to validate if a segment matches the text prompt. <br> - Publishes the structured result (success/failure, mask coordinates) to the processing-results topic.
	Message Queue
	Apache Kafka
	- Decouples the backend API from the heavy processing module. &lt;br> - Manages two topics: snapshot-requests and processing-results. &lt;br> - Ensures reliable, asynchronous communication.
	Database
	MongoDB
	- Stores structured documents for each processing result. &lt;br> - Provides a "delete all" functionality.
	AI Service
	Google Vertex AI
	- Provides powerful multimodal models to understand the content of image segments based on a text prompt.
	4. Data Flow & Logic
4.1. Step 1: Initial Object Detection
1. User Action: User enters "People's face" as the prompt, selects a mask format, and clicks "Start".
2. Frontend: Accesses the camera and displays the feed in "Realtime Camera". It starts a timer to capture a snapshot every x seconds.
3. Snapshot to Backend: The frontend captures a frame, Base64 encodes it, and sends it to the Backend's /api/detect endpoint along with the prompt. The system must cache the camera real time streaming content, until the snapshot analysis result comes back.
4. Backend to Kafka: The Backend receives the request, packages the image and prompt into a JSON message, and produces it to the snapshot-requests Kafka topic.
5. Processing Module: The module consumes the message from Kafka.
   * It first calls Google Vertex AI Gemini API with the snapshot image and the prompt (i.e. “Does this image contain a person’s face ?”). 
   * If Vertex AI result shows yes, then it will run the Segment Anything Model (SAM) on the image to generate a set of potential object masks., and then ask Gemini (via Vertex AI API) that which object is the expected target.
   * If Vertex AI result shows yes, then it will call the `segment-anything` service (e.g., via an HTTP request like `POST http://segment-anything:8001/segment` with the image data) to generate a set of potential object masks. It then asks Gemini (via Vertex AI API) which of these masks corresponds to the expected target.
   * If the initial Vertex AI check (for object presence) result shows no, or if no suitable mask is validated, then the system will throw the streaming cache for this interval (till the next snapshot timestamp).
6. Result to Kafka: If Vertex AI successfully finds which masks are the right object, then the Processing Module publishes a success message to the processing-results topic. This message includes the original frame's identifier, the prompt, the positive result, and the coordinates of the validated mask. There could be multiple masks identified.
4.2. Step 2: Real-time Tracking
1. Backend Receives Result: The Backend, subscribed to processing-results, receives the success message and the initial mask.
2. Switch to Tracking Mode: The Backend signals the Frontend (e.g., via a WebSocket message) to switch to tracking mode from the target snapshot’s timestamp. 
3. Frontend Video Stream: The Frontend stops sending periodic snapshots and instead establishes a WebSocket connection to the Backend, streaming frames at a higher rate (e.g., 15-30 FPS).
4. Backend Tracking & Processing:
   * For each frame received via the WebSocket, the Backend uses an efficient OpenCV tracking algorithm (e.g., CSRT, KCF) initialized with the mask from Step 1.
   * It calculates the new position of the object in the current frame.
   * It applies the user-selected mask (grey out or border) to the frame.
   * It calculates the frame processing delay: delay = current_timestamp - frame_timestamp.
5. Backend Streams Back: The Backend sends a message back to the Frontend via a return WebSocket channel containing:
   * The processed frame (with mask).
   * The corresponding original frame.
   * The calculated current_delay.
6. Frontend Display:
   * The Frontend displays the processed frame in "Processed Video".
   * It displays the corresponding original frame in "Control Video".
   * It updates the "Current Delay" and recalculates the "Average Delay".
5. API and Data Schema
5.1. Backend REST API Endpoints
Method
	Endpoint
	Description
	POST
	/api/detect
	Frontend sends an initial snapshot for detection. Body: { "image": "<base64_string>", "prompt": "a blue car", "sessionId": "<uuid>" }
	DELETE
	/api/results
	Deletes all documents from the processing_results collection in MongoDB.
	5.2. WebSocket Events
* Client -> Server:
   * 'stream_frame': Sends a new video frame for tracking. Payload: { "image": "<base64_string>", "timestamp": "<iso_8601>" }
* Server -> Client:
   * 'processed_frame': Sends the processed result. Payload: { "processedImage": "<base64_string>", "controlImage": "<base64_string>", "delay": 0.12, "timestamp": "<iso_8601>" }
   * 'tracking_started': Notifies the client that an object has been found and tracking is beginning.
5.3. Kafka Message Formats
* snapshot-requests topic:
* JSON
* 
{
  "imageId": "uuid-v4-string",
  "imageData": "<base64_string>",
  "prompt": "People's face",
  "requestTimestamp": "iso_8601_timestamp"
}
* * 

* processing-results topic:
* JSON
* 
{
  "imageId": "uuid-v4-string",
  "prompt": "People's face",
  "isFound": true,
  "maskCoordinates": [[x1, y1], [x2, y2], ...],
  "processingTimestamp": "iso_8601_timestamp"
}
   *    * 

5.4. MongoDB Schema (processing_results collection)
JSON

{
  "_id": ObjectId("..."),
  "imageId": "uuid-v4-string",
  "prompt": "People's face",
  "isFound": true,
  "maskCoordinates": [[x1, y1], [x2, y2], ...],
  "detectionLatencyMs": 1250,
  "createdAt": ISODate("...")
}


6. Local Development Setup (Docker)
A docker-compose.yml file will be the cornerstone of the local development environment.
      * Services:
      * frontend: Builds from a Dockerfile in the /frontend directory. Runs a Node.js development server.
      * backend: Builds from a Dockerfile in the /backend directory. Runs a Python API server (e.g., Uvicorn for FastAPI).
      * processing-worker: Builds from a Dockerfile in the /processing directory. Runs the Python Kafka consumer script.
      * segment-anything: Uses the `supervisely/segment-anything-2` image, exposing an HTTP endpoint (e.g., on port 8001) for the `processing-worker` to call for segmentation.
      * mongo: Uses the official mongo image.
      * kafka: Uses a standard Kafka image (e.g., from Bitnami or Confluent).
      * zookeeper: A required dependency for Kafka, using a standard image.
      * Networking: All services will be connected via a custom Docker bridge network, allowing them to communicate using their service names as hostnames (e.g., http://backend:8000).
      * Volumes: Code directories will be mounted as volumes to enable hot-reloading for the frontend and backend services during development.
      * Environment Variables: A .env file will manage configuration details like JSON style service account key for Vertex AI access, database connection strings, and Kafka broker addresses.